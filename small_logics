import os
import re
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_VERSION = os.getenv("OPENAI_VERSION")
OPENAI_DEPLOYMENT = os.getenv("OPENAI_DEPLOYMENT")
OPENAI_DEPLOYMENT_ENDPOINT = os.getenv("OPENAI_BASE")

PATH_TO_VECTORSTORE = r"C:\Users\342534\Desktop\Telecom Standards Management\backend\vectorstores"

embeddings = AzureOpenAIEmbeddings(
    api_key=OPENAI_API_KEY,
    api_version=OPENAI_API_VERSION,
    azure_deployment="text-embedding-ada-002",
    azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT
)

def cosine_score(question, answer, context):
    # Initialize embedding model
    emb = AzureOpenAIEmbeddings(
        api_key=OPENAI_API_KEY,
        api_version=OPENAI_API_VERSION,
        azure_deployment="text-embedding-ada-002",
        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT
    )
    vecs = emb.embed_documents([question, answer, context])
    question_vec, answer_vec, context_vec = np.array(vecs[0]), np.array(vecs[1]), np.array(vecs[2])
    context_sim = cosine_similarity([question_vec], [context_vec])[0][0]
    answer_sim = cosine_similarity([question_vec], [answer_vec])[0][0]
    score = (0.7 * context_sim + 0.3 * answer_sim)
    score = (score + 1) / 2 * 100  # Normalize -1..1 to 0..100
    return round(min(max(score, 0), 100), 2)

def already_ingested(standard_name, standard_version, persist_dir):
    vectordb = Chroma(
        collection_name="standards_collection",
        embedding_function=embeddings,
        persist_directory=persist_dir,
    )
    results = vectordb.get(
        where={
            "$and": [
                {"standard_name": standard_name},
                {"standard_version": standard_version}
            ]
        }
    )
    return len(results['ids']) > 0

def ingest_text(raw_text: str, metadata: dict = None, persist_dir: str = PATH_TO_VECTORSTORE):
    standard_name = metadata.get("standard_name")
    standard_version = metadata.get("standard_version")
    if already_ingested(standard_name, standard_version, persist_dir):
        print(f"❌ Standard '{standard_name}' version '{standard_version}' already ingested. Skipping.")
        return
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_text(raw_text)
    docs = []
    for idx, chunk in enumerate(chunks):
        chunk_metadata = (metadata or {}).copy()
        chunk_metadata['chunk_id'] = idx
        docs.append(Document(page_content=chunk, metadata=chunk_metadata))

    vectordb = Chroma(
        collection_name="standards_collection",
        embedding_function=embeddings,
        persist_directory=persist_dir,
    )
    vectordb.add_documents(docs)
    print(f"✅ Ingested {len(docs)} chunks into ChromaDB")


def build_rag_chain(persist_dir: str = PATH_TO_VECTORSTORE):
    llm = AzureChatOpenAI(
        api_key=OPENAI_API_KEY,
        api_version=OPENAI_API_VERSION,
        azure_deployment=OPENAI_DEPLOYMENT,
        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,
        temperature=0
    )

    vectordb = Chroma(
        collection_name="standards_collection",
        embedding_function=embeddings,
        persist_directory=persist_dir,
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    system_prompt = (
    "You are a helpful AI assistant specialized in telecom standards (3GPP and related). "
    "When the user asks about telecom or standards, use the provided context to answer. "
    "If the answer is not in the context or not related to telecom, respond ONLY with **OUT OF CONTEXT QUESTION** (in bold and uppercase). After that, add a short prompt like: 'Would you like to know anything about telecom standards?' "
    "If the question is general chit-chat or not related to telecom, answer naturally using your own knowledge.\n\n"
    "Context: {context}")
 
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(retriever, combine_docs_chain)

# --- This is the only NEW function you need for your FastAPI router! ---
def build_rag_chain_and_retriever(persist_dir: str = PATH_TO_VECTORSTORE):
    """
    Returns (rag_chain, retriever) tuple for use in FastAPI and elsewhere.
    """
    llm = AzureChatOpenAI(
        api_key=OPENAI_API_KEY,
        api_version=OPENAI_API_VERSION,
        azure_deployment=OPENAI_DEPLOYMENT,
        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,
        temperature=0
    )

    vectordb = Chroma(
        collection_name="standards_collection",
        embedding_function=embeddings,
        persist_directory=persist_dir,
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    system_prompt = (
    "You are a helpful AI assistant specialized in telecom standards (3GPP and related). "
    "When the user asks about telecom or standards, use the provided context to answer. "
    "If the answer is not in the context or not related to telecom, respond ONLY with **OUT OF CONTEXT QUESTION** (in bold and uppercase). After that, add a short prompt like: 'Would you like to know anything about telecom standards?' "
    "If the question is general chit-chat or not related to telecom, answer naturally using your own knowledge.\n\n"
    "Context: {context}")

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)
    return rag_chain, retriever

def is_out_of_context(question, context, threshold=0.7):
    emb = AzureOpenAIEmbeddings(
        api_key=OPENAI_API_KEY,
        api_version=OPENAI_API_VERSION,
        azure_deployment="text-embedding-ada-002",
        azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT
    )
   
    if isinstance(context, list):
        context = "\n".join([doc.page_content if hasattr(doc, "page_content") else str(doc) for doc in context])
    vecs = emb.embed_documents([question, context])
    from sklearn.metrics.pairwise import cosine_similarity
    sim = cosine_similarity([vecs[0]], [vecs[1]])[0][0]
    return sim < threshold

def evaluate_retrieval_accuracy(question, answer, context, llm=None):
    # 1. If the answer is OUT OF CONTEXT, return 0
    if isinstance(answer, str) and "OUT OF CONTEXT QUESTION" in answer.upper():
        print(0)
        return 0

    # 2. Out-of-context detection based on embeddings
    if is_out_of_context(question, context, threshold=0.7):
        print(0)
        return 0

    # ... rest of your LLM grading logic ...
    if llm is None:
        llm = AzureChatOpenAI(
            api_key=OPENAI_API_KEY,
            api_version=OPENAI_API_VERSION,
            azure_deployment=OPENAI_DEPLOYMENT,
            azure_endpoint=OPENAI_DEPLOYMENT_ENDPOINT,
            temperature=0.2
        )

    eval_prompt = (
        "You are an expert evaluator for AI answers to technical questions. "
        "Given the user's question, the AI's answer, and the supporting context, "
        "rate the answer's factual accuracy and grounding in the context on a scale from 0 to 100, "
        "where 100 means perfectly correct and grounded in context, and 0 means completely incorrect or fabricated.\n"
        "IMPORTANT: Do NOT use only 0, 50, or 100. Use the full range and be granular (e.g., 87, 76, 32, etc.).\n"
        "If the question is not addressed by the context, or cannot be answered using the context, always respond with 0.\n"
        "Question: {question}\n"
        "Answer: {answer}\n"
        "Context: {context}\n"
        "Score (0-100):"
    )
    if isinstance(context, list):
        context = "\n".join([doc.page_content if hasattr(doc, "page_content") else str(doc) for doc in context])
    messages = [
        {"role": "system", "content": eval_prompt.format(question=question, answer=answer, context=context)}
    ]
    resp = llm.invoke(messages)
    try:
        match = re.search(r'\b([0-9]{1,3})\b', resp.content)
        score = int(match.group(1)) if match else 0
        score = min(max(score, 0), 100)
    except Exception:
        score = 0

    # Fallback for coarse scores
    if score in (0, 50, 100):
        score = cosine_score(question, answer, context)

    return score

