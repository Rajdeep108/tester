INTENTS = {
    "analyze_latest": "Please analyze the latest standards for me",
    "compare_versions": "Please compare versions of a standard",
    "compare_standards": "Please compare two different standards",
    "compare_standard_versions": "Please compare a standard across two versions",
    "explain_features": "Explain the features of a standard",
    "latest_standard": "Which is the latest standard/version",
}


from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate

def build_intent_classifier(llm):
    intent_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an intent classifier. Given a user query, classify it into one of these intents:\n"
                   "1. analyze_latest\n"
                   "2. compare_versions\n"
                   "3. compare_standards\n"
                   "4. compare_standard_versions\n"
                   "5. explain_features\n"
                   "6. latest_standard\n"
                   "If it does not match, return 'general'.\n\nOnly return the intent keyword, nothing else."),
        ("human", "{query}")
    ])
    return LLMChain(llm=llm, prompt=intent_prompt)


def build_rag_chain(persist_dir: str, embeddings, llm):
    vectordb = Chroma(
        collection_name="standards_collection",
        embedding_function=embeddings,
        persist_directory=persist_dir,
    )
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    system_prompt = (
        "You are a helpful AI assistant specialized in telecom standards (3GPP and related). "
        "When the user asks about telecom or standards, use the provided context to answer. "
        "If the answer is not in the context, but it's still about telecom, say 'I don't know'. "
        "If the question is general chit-chat or not related to telecom, answer naturally using your own knowledge.\n\n"
        "Context: {context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(retriever, combine_docs_chain)


def route_intent(intent, inputs, vectordb, llm, embeddings):
    if intent == "general":
        return llm.invoke(inputs)

    if intent == "latest_standard":
        results = vectordb.get(where={"standard_name": {"$exists": True}})
        latest = sorted(results["metadatas"], key=lambda m: m.get("release_date", ""), reverse=True)[0]
        return {"answer": f"The latest standard is {latest['standard_name']} version {latest['standard_version']} released on {latest['release_date']}."}

    if intent == "compare_versions":
        # Implement version comparison logic here
        pass

    if intent == "compare_standards":
        # Implement standard comparison logic here
        pass

    if intent == "compare_standard_versions":
        # Implement standard version comparison logic here
        pass

    if intent == "explain_features":
        # Implement feature explanation logic here
        pass

    # Default fallback to RAG
    return rag_chain.invoke(inputs)


-----------------------------------------------------------------------------------------------------------------------------------------------------

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

@router.post("/chat/")
async def chat(request: ChatRequest):
    try:
        intent = intent_classifier.run({"query": request.question}).strip()
        result = route_intent(intent, {"input": request.question}, vectordb, llm, embeddings)
        answer = result["answer"]
        insert_chat_history(request.question, answer)
        return ChatResponse(answer=answer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
